{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "926c578f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned folder: D:\\SaaS Ticket Analytics Dashboard\\cleaned\n"
     ]
    }
   ],
   "source": [
    "# Cell 1 — imports & paths\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "project = Path(r\"D:\\SaaS Ticket Analytics Dashboard\")\n",
    "cleaned = project / \"cleaned\"\n",
    "cleaned.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Cleaned folder:\", cleaned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8f0eca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: D:\\SaaS Ticket Analytics Dashboard\\cleaned\\tickets_master_clean.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(11000, 25)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 2 — load tickets CSV\n",
    "src = cleaned / \"tickets_master_clean.csv\"   # if you have a raw source, change this path\n",
    "print(\"Loading:\", src)\n",
    "tickets = pd.read_csv(src, low_memory=False, parse_dates=['created_at','first_response_at','resolved_at'])\n",
    "tickets.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd3e89a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before duplicates: 11000\n",
      "After duplicates: 11000\n"
     ]
    }
   ],
   "source": [
    "# Cell 3 — drop exact duplicates\n",
    "print(\"Before duplicates:\", len(tickets))\n",
    "tickets = tickets.drop_duplicates()\n",
    "print(\"After duplicates:\", len(tickets))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46873e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns normalized. Sample: ['ticket_id', 'created_at', 'first_response_at', 'resolved_at', 'priority', 'status', 'issue_type', 'channel', 'customer_id', 'agent_id', 'first_response_mins', 'resolution_time_hours', 'sla_hours', 'sla_breached', 'reopened', 'escalated', 'csat', 'created_date', 'created_hour', 'created_weekday']\n"
     ]
    }
   ],
   "source": [
    "# Cell 4 — normalize column names\n",
    "tickets.columns = [c.strip().lower().replace(' ', '_') for c in tickets.columns]\n",
    "print(\"Columns normalized. Sample:\", tickets.columns.tolist()[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "304e6c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date columns processed. created_at min/max: 2024-11-01 00:15:17 2025-10-31 22:21:02\n"
     ]
    }
   ],
   "source": [
    "# Cell 5 — date conversions and derived cols\n",
    "tickets['created_at'] = pd.to_datetime(tickets['created_at'], errors='coerce')\n",
    "tickets['first_response_at'] = pd.to_datetime(tickets['first_response_at'], errors='coerce') if 'first_response_at' in tickets.columns else None\n",
    "tickets['resolved_at'] = pd.to_datetime(tickets['resolved_at'], errors='coerce') if 'resolved_at' in tickets.columns else None\n",
    "\n",
    "tickets['created_date'] = tickets['created_at'].dt.date\n",
    "tickets['year_month'] = tickets['created_at'].dt.to_period('M').astype(str)\n",
    "tickets['created_hour'] = tickets['created_at'].dt.hour\n",
    "\n",
    "print(\"Date columns processed. created_at min/max:\", tickets['created_at'].min(), tickets['created_at'].max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a4625e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resolution_time_hours: min/max 1.0 145.89180547916666\n",
      "first_response_mins: min/max 2.0 1174.0\n"
     ]
    }
   ],
   "source": [
    "# Cell 6 — compute resolution_time_hours and first_response_mins\n",
    "# resolution_time_hours = (resolved_at - created_at) in hours\n",
    "if 'resolved_at' in tickets.columns:\n",
    "    tickets['resolution_time_hours'] = (tickets['resolved_at'] - tickets['created_at']).dt.total_seconds() / 3600.0\n",
    "else:\n",
    "    tickets['resolution_time_hours'] = pd.NA\n",
    "\n",
    "# first_response_mins = (first_response_at - created_at) in minutes\n",
    "if 'first_response_at' in tickets.columns:\n",
    "    tickets['first_response_mins'] = (tickets['first_response_at'] - tickets['created_at']).dt.total_seconds() / 60.0\n",
    "else:\n",
    "    tickets['first_response_mins'] = pd.NA\n",
    "\n",
    "# quick sanity\n",
    "print(\"resolution_time_hours: min/max\", tickets['resolution_time_hours'].min(), tickets['resolution_time_hours'].max())\n",
    "print(\"first_response_mins: min/max\", tickets['first_response_mins'].min(), tickets['first_response_mins'].max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54bae552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLA breached value counts:\n",
      " {0: 11000}\n"
     ]
    }
   ],
   "source": [
    "# Cell 7 — normalize sla_breached column\n",
    "# If there's an existing boolean/int/text column, convert, else create from sla_hours if available.\n",
    "if 'sla_breached' in tickets.columns:\n",
    "    tickets['sla_breached'] = tickets['sla_breached'].fillna(0).astype(int)\n",
    "elif 'sla_hours' in tickets.columns and 'resolution_time_hours' in tickets.columns:\n",
    "    tickets['sla_breached'] = (tickets['resolution_time_hours'] > tickets['sla_hours']).astype(int)\n",
    "else:\n",
    "    tickets['sla_breached'] = 0  # default if no SLA info\n",
    "\n",
    "print(\"SLA breached value counts:\\n\", tickets['sla_breached'].value_counts(dropna=False).to_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1941426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSAT stats: count    11000.000000\n",
      "mean         3.560191\n",
      "std          0.727957\n",
      "min          1.000000\n",
      "25%          3.100000\n",
      "50%          3.600000\n",
      "75%          4.100000\n",
      "max          5.000000\n",
      "Name: csat, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Cell 8 — clean csat values\n",
    "if 'csat' in tickets.columns:\n",
    "    tickets['csat'] = pd.to_numeric(tickets['csat'], errors='coerce')\n",
    "    # optional: clip to plausible range 1-5\n",
    "    tickets['csat'] = tickets['csat'].clip(lower=1, upper=5)\n",
    "    print(\"CSAT stats:\", tickets['csat'].describe())\n",
    "else:\n",
    "    print(\"No csat column found; skipping.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46a3b03b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical cleanup done.\n"
     ]
    }
   ],
   "source": [
    "# Cell 9 — clean text categorical fields\n",
    "for col in ['issue_type', 'priority', 'channel', 'agent_name', 'customer_id']:\n",
    "    if col in tickets.columns:\n",
    "        tickets[col] = tickets[col].astype(str).str.strip()\n",
    "print(\"Categorical cleanup done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "806cc2ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 0 rows with missing ticket_id or created_at.\n"
     ]
    }
   ],
   "source": [
    "# Cell 10 — drop rows without ticket_id or created_at\n",
    "before = len(tickets)\n",
    "tickets = tickets.dropna(subset=['ticket_id', 'created_at'])\n",
    "after = len(tickets)\n",
    "print(f\"Dropped {before-after} rows with missing ticket_id or created_at.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4efe59c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tickets: 11000\n",
      "Avg first response mins: 260.39\n",
      "Avg resolution hrs: 20.86\n",
      "SLA breach %: 0.0\n",
      "Avg CSAT: 3.56\n"
     ]
    }
   ],
   "source": [
    "# Cell 11 — recompute simple KPIs\n",
    "print(\"Total tickets:\", len(tickets))\n",
    "print(\"Avg first response mins:\", round(tickets['first_response_mins'].mean(),2))\n",
    "print(\"Avg resolution hrs:\", round(tickets['resolution_time_hours'].mean(),2))\n",
    "print(\"SLA breach %:\", round(tickets['sla_breached'].mean()*100,2) if 'sla_breached' in tickets.columns else \"N/A\")\n",
    "print(\"Avg CSAT:\", round(tickets['csat'].mean(),2) if 'csat' in tickets.columns else \"N/A\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e96c8bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cleaned tickets to: D:\\SaaS Ticket Analytics Dashboard\\cleaned\\tickets_master_clean.csv\n"
     ]
    }
   ],
   "source": [
    "# Cell 12 — save cleaned tickets CSV\n",
    "out_tickets = cleaned / \"tickets_master_clean.csv\"\n",
    "tickets.to_csv(out_tickets, index=False)\n",
    "print(\"Saved cleaned tickets to:\", out_tickets)\n",
    "\n",
    "# If you have agents/customers DataFrames loaded earlier and want to save cleaned versions:\n",
    "# agents.to_csv(cleaned / \"agents_clean.csv\", index=False)\n",
    "# customers.to_csv(cleaned / \"customers_clean.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86f890f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote cleaning_log.txt\n"
     ]
    }
   ],
   "source": [
    "# Cell 13 — write a simple cleaning log\n",
    "log_text = f\"\"\"\n",
    "Basic cleaning actions applied:\n",
    "- dropped duplicates\n",
    "- normalized column names (lowercase, underscores)\n",
    "- parsed created_at, first_response_at, resolved_at\n",
    "- derived: created_date, year_month, created_hour\n",
    "- computed: resolution_time_hours, first_response_mins\n",
    "- ensured sla_breached is binary (0/1)\n",
    "- csat coerced to numeric and clipped 1-5 (if present)\n",
    "Saved cleaned tickets to: {out_tickets}\n",
    "\"\"\"\n",
    "with open(cleaned / \"cleaning_log.txt\", \"w\", encoding=\"utf-8\") as fh:\n",
    "    fh.write(log_text.strip())\n",
    "print(\"Wrote cleaning_log.txt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
